{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64af477f",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing Pipeline\n",
    "\n",
    "## Key Features\n",
    "- **Overture Maps download** via DuckDB with bounding box filtering (outputs GeoParquet)\n",
    "- **FlatGeobuf conversion** for optimal tile generation (streaming read, spatial indexing)\n",
    "- **Multi-format conversion** (Shapefile, GeoPackage, etc.) to GeoJSON\n",
    "- **Automated PMTiles generation** with tippecanoe\n",
    "- **Performance optimized** for continent/world-scale processing\n",
    "\n",
    "## Processing Steps\n",
    "1. **Download** - Fetch Overture Maps data for specified extent (as GeoParquet)\n",
    "2. **Convert to FlatGeobuf** - Transform GeoParquet to FlatGeobuf for efficient tiling\n",
    "3. **Convert Custom Data** - Transform custom spatial data to GeoJSON/FlatGeobuf\n",
    "4. **Tile** - Generate PMTiles using tippecanoe with optimized settings\n",
    "\n",
    "## Format Optimization Strategy\n",
    "- **GeoParquet (.parquet)** - Download format (compact, fast DuckDB queries)\n",
    "- **FlatGeobuf (.fgb)** - Tiling format (streaming, spatial index, native tippecanoe support)\n",
    "- **GeoJSON (.geojson)** - Legacy support for small datasets\n",
    "\n",
    "### Why FlatGeobuf for Large-Scale Processing?\n",
    "- ✓ **Streaming read**: Process datasets larger than memory\n",
    "- ✓ **Spatial indexing**: Built-in R-tree for fast spatial queries\n",
    "- ✓ **Compact**: 30-50% smaller than GeoJSON\n",
    "- ✓ **Fast**: Optimized for millions of features\n",
    "- ✓ **Native tippecanoe support**: v2.17+\n",
    "\n",
    "## Prerequisites\n",
    "- Python with required packages (duckdb, geopandas, tqdm, pathlib)\n",
    "- Tippecanoe 2.17.0+ installed and available in PATH\n",
    "- GDAL/OGR for geospatial format conversion\n",
    "- PyArrow for GeoParquet processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e8a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded environment from repository root: /Users/matthewheaton/GitHub/.env\n",
      "  DATA_DISK = .\n",
      "\n",
      "=== CONFIGURATION VERIFICATION ===\n",
      "Repository root:       /Users/matthewheaton/GitHub\n",
      "Environment .env:      /Users/matthewheaton/GitHub/.env\n",
      "Environment DATA_DISK: .\n",
      "Config uses:           /Users/matthewheaton/GitHub/basemap\n",
      "PROJECT CONFIGURATION\n",
      "============================================================\n",
      "Project root:        /Users/matthewheaton/GitHub/basemap/1-processing\n",
      "Scripts directory:   /Users/matthewheaton/GitHub/basemap/1-processing/scripts\n",
      "Notebooks directory: /Users/matthewheaton/GitHub/basemap/1-processing/notebooks\n",
      "Data directory:      /Users/matthewheaton/GitHub/basemap/data\n",
      "Scratch directory:   /Users/matthewheaton/GitHub/basemap/data/2-scratch\n",
      "Output directory:    /Users/matthewheaton/GitHub/basemap/data/3-pmtiles\n",
      "Overture data:       /Users/matthewheaton/GitHub/basemap/data/1-input/overture\n",
      "GRID3 data:         /Users/matthewheaton/GitHub/basemap/data/1-input/grid3\n",
      "\n",
      "Processing extent:   (27.0, -8.0, 30.5, -2.0)\n",
      "Buffer degrees:      0\n",
      "Area:                21.0000 degree² (~258741 km²)\n",
      "============================================================\n",
      "\n",
      "✓ Configuration loaded - CONFIG available in all cells\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Run this cell first\n",
    "# ============================================================\n",
    "# This cell initializes all configuration and should be run \n",
    "# first. Re-run this cell to reload configuration changes.\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "processing_dir = notebook_dir.parent  # 1-processing\n",
    "repo_root = processing_dir.parent     # basemap (repository root)\n",
    "\n",
    "# Add processing directory to path\n",
    "if str(processing_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(processing_dir))\n",
    "\n",
    "# Load environment variables from REPOSITORY ROOT (monorepo-wide .env)\n",
    "env_path = repo_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "print(f\"✓ Loaded environment from repository root: {env_path}\")\n",
    "print(f\"  DATA_DISK = {os.environ.get('DATA_DISK', 'not set')}\")\n",
    "\n",
    "# Import configuration (will also load .env via config.py)\n",
    "from config import (\n",
    "    get_config,\n",
    "    ensure_directories,\n",
    "    print_config_summary,\n",
    "    SCRIPTS_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    OVERTURE_DATA_DIR,\n",
    "    GRID3_DATA_DIR,\n",
    "    SCRATCH_DIR,\n",
    ")\n",
    "\n",
    "# Import processing functions\n",
    "from scripts import (\n",
    "    download_overture_data,\n",
    "    convert_file,\n",
    "    convert_parquet_to_fgb,\n",
    "    batch_convert_directory,\n",
    "    process_to_tiles,\n",
    "    create_tilejson,\n",
    ")\n",
    "\n",
    "# Additional libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL CONFIGURATION - Available in all cells below\n",
    "# ============================================================\n",
    "CONFIG = get_config()\n",
    "\n",
    "# Customize extent\n",
    "CONFIG[\"extent\"][\"coordinates\"] = (\n",
    "    27.0,   # lon_min (west of Haut-Lomami)\n",
    "    -8.0,  # lat_min (southern extent)\n",
    "    30.5,   # lon_max (east of Tanganyika)\n",
    "    -2.0    # lat_max (northern extent)\n",
    ")\n",
    "CONFIG[\"extent\"][\"buffer_degrees\"] = 0\n",
    "\n",
    "# Processing options\n",
    "CONFIG[\"tiling\"][\"input_dirs\"] = [SCRATCH_DIR]  # Read FlatGeobuf files from scratch\n",
    "CONFIG[\"download\"][\"verbose\"] = True\n",
    "CONFIG[\"conversion\"][\"verbose\"] = True\n",
    "CONFIG[\"tiling\"][\"verbose\"] = True\n",
    "CONFIG[\"tiling\"][\"parallel\"] = True\n",
    "\n",
    "# Create directories and verify\n",
    "ensure_directories()\n",
    "\n",
    "# Verification\n",
    "print(\"\\n=== CONFIGURATION VERIFICATION ===\")\n",
    "print(f\"Repository root:       {repo_root}\")\n",
    "print(f\"Environment .env:      {env_path}\")\n",
    "print(f\"Environment DATA_DISK: {os.environ.get('DATA_DISK', 'NOT SET')}\")\n",
    "print(f\"Config uses:           {CONFIG['paths']['data_dir'].parent}\")\n",
    "\n",
    "env_disk = Path(os.environ.get('DATA_DISK', ''))\n",
    "config_disk = CONFIG['paths']['data_dir'].parent\n",
    "# if env_disk and env_disk == config_disk:\n",
    "#     print(\"✓ Paths synchronized between .env and config.py\\n\")\n",
    "# else:\n",
    "#     print(f\"⚠ WARNING: Path mismatch!\")\n",
    "#     print(f\"  .env sets:      {env_disk}\")\n",
    "#     print(f\"  config.py uses: {config_disk}\")\n",
    "#     print(f\"  Fix: Restart kernel to reload config.py with new environment\\n\")\n",
    "\n",
    "print_config_summary(CONFIG)\n",
    "print(\"\\n✓ Configuration loaded - CONFIG available in all cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea97440",
   "metadata": {},
   "source": [
    "## 2. Download Overture Data with DuckDB\n",
    "\n",
    "Use the `downloadOverture.py` module to fetch geospatial data from Overture Maps. This module uses DuckDB to efficiently query and download data for specific geographic extents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b170545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Overture Maps data\n",
    "print(\"=== STEP 1: DOWNLOADING OVERTURE DATA ===\")\n",
    "download_results = download_overture_data(\n",
    "    extent=CONFIG[\"extent\"][\"coordinates\"],\n",
    "    buffer_degrees=CONFIG[\"extent\"][\"buffer_degrees\"],\n",
    "    template_path=str(CONFIG[\"paths\"][\"template_path\"]),\n",
    "    verbose=CONFIG[\"download\"][\"verbose\"],\n",
    "    project_root=str(CONFIG[\"paths\"][\"project_root\"]),\n",
    "    overture_data_dir=str(CONFIG[\"paths\"][\"overture_data_dir\"])\n",
    ")\n",
    "\n",
    "print(f\"Download completed: {download_results['success']}\")\n",
    "print(f\"Sections processed: {download_results['processed_sections']}\")\n",
    "if download_results[\"errors\"]:\n",
    "    print(f\"Errors encountered: {len(download_results['errors'])}\")\n",
    "    for error in download_results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f11f7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files were created during download\n",
    "print(\"=== CHECKING DOWNLOADED FILES ===\")\n",
    "\n",
    "overture_files = []\n",
    "search_dirs = [CONFIG[\"paths\"][\"data_dir\"], CONFIG[\"paths\"][\"overture_data_dir\"]]\n",
    "\n",
    "for data_dir in search_dirs:\n",
    "    if data_dir.exists():\n",
    "        for pattern in CONFIG[\"download\"][\"output_formats\"]:\n",
    "            files = list(data_dir.glob(pattern))\n",
    "            overture_files.extend(files)\n",
    "\n",
    "print(f\"Found {len(overture_files)} downloaded files:\")\n",
    "for file in sorted(overture_files):\n",
    "    file_size = file.stat().st_size / 1024 / 1024  # Size in MB\n",
    "    print(f\"  {file.name} ({file_size:.1f} MB)\")\n",
    "\n",
    "# Display file statistics\n",
    "if overture_files:\n",
    "    total_size_mb = sum(f.stat().st_size for f in overture_files) / 1024 / 1024\n",
    "    print(f\"\\nTotal size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"Search directories: {[str(d) for d in search_dirs]}\")\n",
    "else:\n",
    "    print(\"No files found. Check download results above.\")\n",
    "    print(f\"Searched in: {[str(d) for d in search_dirs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89d40a",
   "metadata": {},
   "source": [
    "## 2.5. Convert GeoParquet to FlatGeobuf for Optimal Tiling\n",
    "\n",
    "Convert downloaded GeoParquet files to FlatGeobuf format for efficient tile generation.\n",
    "\n",
    "### Why This Step?\n",
    "- **Memory efficiency**: FlatGeobuf supports streaming reads (essential for large datasets)\n",
    "- **Speed**: Built-in spatial indexing accelerates tippecanoe processing\n",
    "- **Native support**: Tippecanoe 2.17+ reads FlatGeobuf natively (no intermediate conversion)\n",
    "- **Compact**: 30-50% smaller than GeoJSON while maintaining full attribute data\n",
    "\n",
    "### Performance for Large Datasets\n",
    "- **Continent-scale**: Process billions of features without memory issues\n",
    "- **World-scale**: Optimal format for global basemap generation\n",
    "- **Parallel-friendly**: Each file can be processed independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoParquet files to FlatGeobuf for optimal tiling performance\n",
    "print(\"=== STEP 2.5: CONVERTING GEOPARQUET TO FLATGEOBUF ===\")\n",
    "\n",
    "# Use CONFIG settings for FlatGeobuf conversion\n",
    "fgb_results = batch_convert_directory(\n",
    "    input_dir=str(CONFIG[\"paths\"][\"overture_data_dir\"]),\n",
    "    output_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Save FGB files to scratch directory\n",
    "    pattern=CONFIG[\"fgb_conversion\"][\"input_pattern\"],\n",
    "    overwrite=CONFIG[\"fgb_conversion\"][\"overwrite\"],\n",
    "    verbose=CONFIG[\"fgb_conversion\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nConversion Summary:\")\n",
    "print(f\"  ✓ Converted: {fgb_results['converted']} files\")\n",
    "print(f\"  ⊘ Skipped:   {fgb_results['skipped']} files (already exist)\")\n",
    "print(f\"  ✗ Errors:    {len(fgb_results['errors'])} files\")\n",
    "\n",
    "if fgb_results['errors']:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in fgb_results['errors']:\n",
    "        print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "if fgb_results['output_files']:\n",
    "    print(f\"\\n✓ FlatGeobuf files ready for tippecanoe\")\n",
    "    print(f\"  Location: {CONFIG['paths']['scratch_dir']}\")\n",
    "else:\n",
    "    print(f\"\\nNo new FlatGeobuf files created.\")\n",
    "    if fgb_results['skipped'] > 0:\n",
    "        print(f\"All {fgb_results['skipped']} files already converted. Use overwrite=True to reconvert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacbe99",
   "metadata": {},
   "source": [
    "## 3. Convert Custom Spatial Data for Tippecanoe\n",
    "\n",
    "Use the `convertCustomData.py` module to convert various geospatial formats to newline-delimited GeoJSON files suitable for Tippecanoe \n",
    "\n",
    "### Supported Input Formats\n",
    "- Shapefile (.shp)\n",
    "- GeoPackage (.gpkg)\n",
    "- FileGDB (.gdb)\n",
    "- SQLite/SpatiaLite (.sqlite, .db)\n",
    "- PostGIS (connection string)\n",
    "- CSV with geometry columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for custom data files to convert\n",
    "print(\"=== STEP 3: CONVERTING CUSTOM SPATIAL DATA ===\")\n",
    "\n",
    "custom_input_dir = CONFIG[\"paths\"][\"grid3_data_dir\"]\n",
    "custom_files = []\n",
    "\n",
    "# Search for various spatial data formats using CONFIG patterns\n",
    "for pattern in CONFIG[\"conversion\"][\"input_patterns\"]:\n",
    "    custom_files.extend(custom_input_dir.glob(pattern))\n",
    "\n",
    "print(f\"Found {len(custom_files)} custom data files to convert:\")\n",
    "print(f\"Search directory: {custom_input_dir}\")\n",
    "for file in custom_files:\n",
    "    print(f\"  {file.name}\")\n",
    "\n",
    "# Convert custom data files (if any exist)\n",
    "converted_files = []\n",
    "\n",
    "for input_file in custom_files:\n",
    "    output_file = CONFIG[\"paths\"][\"output_dir\"] / f\"{input_file.stem}{CONFIG['conversion']['output_suffix']}\"\n",
    "    \n",
    "    print(f\"Converting {input_file.name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert using the modular function with CONFIG settings\n",
    "        processed, skipped, output_path = convert_file(\n",
    "            input_path=str(input_file),\n",
    "            output_path=str(output_file),\n",
    "            reproject=CONFIG[\"conversion\"][\"reproject_crs\"],\n",
    "            verbose=CONFIG[\"conversion\"][\"verbose\"]\n",
    "        )\n",
    "        \n",
    "        converted_files.append(output_file)\n",
    "        print(f\"✓ Converted: {processed} features, {skipped} skipped\")\n",
    "        print(f\"  Output: {output_file.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error converting {input_file.name}: {e}\")\n",
    "\n",
    "if converted_files:\n",
    "    print(f\"\\n✓ Successfully converted {len(converted_files)} files\")\n",
    "    print(f\"  Output directory: {CONFIG['paths']['output_dir']}\")\n",
    "else:\n",
    "    print(f\"\\nNo custom files to convert. Add data files to: {custom_input_dir}\")\n",
    "    print(f\"Supported formats: {', '.join(CONFIG['conversion']['input_patterns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a45ef5",
   "metadata": {},
   "source": [
    "## 3 1/2. Define tippecanoe parameters per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a0d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60968d93",
   "metadata": {},
   "source": [
    "## 4. Process FlatGeobuf to PMTiles\n",
    "\n",
    "Use the `runCreateTiles.py` module to convert FlatGeobuf files to PMTiles using optimized Tippecanoe settings.\n",
    "\n",
    "### Supported Input Formats (Priority Order)\n",
    "1. **FlatGeobuf (.fgb)** - **RECOMMENDED** for large-scale processing\n",
    "   - Streaming read capability (low memory)\n",
    "   - Built-in spatial indexing (fast)\n",
    "   - Native tippecanoe support\n",
    "   - Optimal for continent/world-scale data\n",
    "\n",
    "2. **GeoJSONSeq (.geojsonseq)** - Good for medium datasets\n",
    "   - Line-delimited format\n",
    "   - Sequential processing\n",
    "\n",
    "3. **GeoJSON (.geojson)** - Small datasets only\n",
    "   - Full file must load into memory\n",
    "   - Not recommended for large areas\n",
    "\n",
    "### Automatic Optimization Features\n",
    "- **Geometry Detection**: Automatically detects Point, LineString, or Polygon geometries\n",
    "- **Layer-Specific Settings**: Optimized settings for water, roads, places, land use, etc.\n",
    "- **Parallel Processing**: Multi-threaded processing for large datasets\n",
    "- **Quality Optimization**: Smart simplification and feature dropping\n",
    "- **Format Detection**: Automatically selects best input format available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a575ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 4: PROCESSING TO PMTILES ===\n",
      "=== PROCESSING TO TILES ===\n",
      "Found 1 files to process:\n",
      "  water.fgb (FlatGeobuf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/1 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using template settings for water.fgb (8 options)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1/1 [00:08<00:00,  8.23s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ water.fgb -> /Users/matthewheaton/GitHub/basemap/data/3-pmtiles/water.pmtiles\n",
      "\n",
      "=== TILE PROCESSING COMPLETE ===\n",
      "Processed: 1/1 files\n",
      "\n",
      "✓ Successfully generated 1 PMTiles:\n",
      "  buildings.pmtiles (288.4 MB)\n",
      "  infrastructure.pmtiles (1.5 MB)\n",
      "  land.pmtiles (23.4 MB)\n",
      "  land_cover.pmtiles (198.2 MB)\n",
      "  land_residential.pmtiles (10.7 MB)\n",
      "  land_use.pmtiles (13.0 MB)\n",
      "  roads.pmtiles (220.4 MB)\n",
      "  water.pmtiles (20.3 MB)\n",
      "\n",
      "Total PMTiles size: 775.9 MB\n",
      "Files location: /Users/matthewheaton/GitHub/basemap/data/3-pmtiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Process all geospatial files to PMTiles\n",
    "print(\"=== STEP 4: PROCESSING TO PMTILES ===\")\n",
    "\n",
    "# Process all downloaded and converted files to PMTiles using CONFIG settings\n",
    "# Now supports: GeoJSON, GeoJSONSeq, and GeoParquet formats\n",
    "tiling_results = process_to_tiles(\n",
    "    extent=CONFIG[\"extent\"][\"coordinates\"],\n",
    "    input_dirs=[str(d) for d in CONFIG[\"tiling\"][\"input_dirs\"]],  # Convert Path objects to strings\n",
    "    filter_pattern=CONFIG[\"tiling\"][\"filter_pattern\"],  # Pass filter pattern from CONFIG\n",
    "    output_dir=str(CONFIG[\"tiling\"][\"output_dir\"]),  # Use explicit output directory from CONFIG\n",
    "    parallel=CONFIG[\"tiling\"][\"parallel\"],\n",
    "    verbose=CONFIG[\"tiling\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "# print(f\"Tiling completed: {tiling_results['success']}\")\n",
    "# print(f\"Files processed: {len(tiling_results['processed_files'])}/{tiling_results['total_files']}\")\n",
    "\n",
    "if tiling_results[\"errors\"]:\n",
    "    print(f\"Errors encountered: {len(tiling_results['errors'])}\")\n",
    "    for error in tiling_results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Display generated PMTiles files\n",
    "if tiling_results[\"processed_files\"]:\n",
    "    print(f\"\\n✓ Successfully generated {len(tiling_results['processed_files'])} PMTiles:\")\n",
    "    \n",
    "    pmtiles_files = list(CONFIG[\"paths\"][\"tile_dir\"].glob(\"*.pmtiles\"))\n",
    "    \n",
    "    total_size_mb = 0\n",
    "    for pmtile in sorted(pmtiles_files):\n",
    "        size_mb = pmtile.stat().st_size / 1024 / 1024\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"  {pmtile.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal PMTiles size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"Files location: {CONFIG['paths']['tile_dir']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo PMTiles files were generated. Check the errors above.\")\n",
    "    print(f\"Make sure you have geospatial files (GeoJSON/GeoJSONSeq/GeoParquet) in: {[str(d) for d in CONFIG['tiling']['input_dirs']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f46e1",
   "metadata": {},
   "source": [
    "## 5. Create TileJSON Metadata\n",
    "\n",
    "Generate TileJSON metadata files for seamless integration with web mapping libraries like MapLibre GL JS.\n",
    "\n",
    "### TileJSON Features\n",
    "- **Bounds and zoom levels** automatically detected from PMTiles\n",
    "- **Vector layer definitions** for each data layer\n",
    "- **MapLibre GL JS compatibility** for easy web integration\n",
    "- **PMTiles URL references** for efficient tile serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57093621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create TileJSON metadata for MapLibre integration\n",
    "print(\"=== STEP 5: CREATING TILEJSON METADATA ===\")\n",
    "\n",
    "# Check if PMTiles files exist in the configured tile directory\n",
    "pmtiles_files = list(CONFIG[\"paths\"][\"tile_dir\"].glob(\"*.pmtiles\"))\n",
    "\n",
    "if pmtiles_files:\n",
    "    print(f\"Found {len(pmtiles_files)} PMTiles files, creating TileJSON...\")\n",
    "    \n",
    "    try:\n",
    "        tilejson = create_tilejson(\n",
    "            tile_dir=str(CONFIG[\"paths\"][\"tile_dir\"]),  # Explicitly pass tile directory\n",
    "            extent=CONFIG[\"extent\"][\"coordinates\"],  # Pass extent from CONFIG\n",
    "            output_file=str(CONFIG[\"paths\"][\"tile_dir\"] / \"tilejson.json\")  # Explicitly pass output file path\n",
    "        )\n",
    "        \n",
    "        print(\"✓ TileJSON created successfully\")\n",
    "        print(f\"  Bounds: {tilejson['bounds']}\")\n",
    "        print(f\"  Zoom range: {tilejson['minzoom']} - {tilejson['maxzoom']}\")\n",
    "        print(f\"  Vector layers: {len(tilejson['vector_layers'])}\")\n",
    "        print(f\"  Output file: {CONFIG['paths']['tile_dir'] / 'tilejson.json'}\")\n",
    "        \n",
    "        # Show a summary of all output files\n",
    "        print(f\"\\nComplete output summary:\")\n",
    "        total_size_mb = 0\n",
    "        for pmtile in sorted(pmtiles_files):\n",
    "            size_mb = pmtile.stat().st_size / 1024 / 1024\n",
    "            total_size_mb += size_mb\n",
    "            print(f\"  {pmtile.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        print(f\"  tilejson.json\")\n",
    "        print(f\"\\nTotal PMTiles size: {total_size_mb:.1f} MB\")\n",
    "        print(f\"All files location: {CONFIG['paths']['tile_dir']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ TileJSON creation failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No PMTiles files found in output directory.\")\n",
    "    print(f\"Expected location: {CONFIG['paths']['tile_dir']}\")\n",
    "    print(\"Run Step 4 first to generate PMTiles files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755057e5",
   "metadata": {},
   "source": [
    "## 6. Validate and Test Individual Steps\n",
    "\n",
    "Test each processing step individually and validate the generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Step Testing and Validation\n",
    "\n",
    "print(\"INDIVIDUAL STEP TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Test downloadOverture.py standalone:\")\n",
    "print(\"python processing/downloadOverture.py --extent='23.4,-6.2,23.8,-5.8' --buffer=0.1\")\n",
    "\n",
    "print(\"\\n2. Test convertCustomData.py standalone:\")\n",
    "print(\"python processing/convertCustomData.py input.shp output.geojsonseq --reproject=EPSG:4326\")\n",
    "\n",
    "print(\"\\n3. Test runCreateTiles.py standalone:\")\n",
    "print(\"python processing/runCreateTiles.py --extent='23.4,-6.2,23.8,-5.8' --create-tilejson\")\n",
    "\n",
    "print(\"\\n4. Test individual steps in this notebook:\")\n",
    "print(\"   - Step 1: Download section (cell 6)\")\n",
    "print(\"   - Step 2: Check downloaded files (cell 7)\")\n",
    "print(\"   - Step 3: Convert custom data (cell 9)\")\n",
    "print(\"   - Step 4: Process to PMTiles (cell 11)\")\n",
    "print(\"   - Step 5: Create TileJSON (cell 13)\")\n",
    "\n",
    "print(\"\\n5. Validate outputs using CONFIG paths:\")\n",
    "print(f\"   - Check {CONFIG['paths']['data_dir']} for GeoJSON files\")\n",
    "print(f\"   - Check {CONFIG['paths']['tile_dir']} for PMTiles files\")\n",
    "print(f\"   - Verify TileJSON metadata file\")\n",
    "\n",
    "# Configuration validation using centralized CONFIG\n",
    "print(\"\\nCURRENT CONFIGURATION VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Extent: {CONFIG['extent']['coordinates']}\")\n",
    "print(f\"Buffer: {CONFIG['extent']['buffer_degrees']} degrees\")\n",
    "print(f\"Tile output directory: {CONFIG['paths']['tile_dir']}\")\n",
    "print(f\"Custom data directory: {CONFIG['paths']['grid3_data_dir']}\")\n",
    "print(f\"Input directories for tiling: {[str(d) for d in CONFIG['tiling']['input_dirs']]}\")\n",
    "\n",
    "# Area calculation using CONFIG\n",
    "extent = CONFIG['extent']['coordinates']\n",
    "area = (extent[2] - extent[0]) * (extent[3] - extent[1])\n",
    "print(f\"Processing area: {area:.2f} degree² ({area * 111**2:.0f} km²)\")\n",
    "\n",
    "# Check directory status\n",
    "print(f\"\\nDIRECTORY STATUS\")\n",
    "print(\"=\" * 30)\n",
    "for path_name, path_obj in CONFIG['paths'].items():\n",
    "    if path_name.endswith('_dir'):\n",
    "        status = \"exists\" if path_obj.exists() else \"missing\"\n",
    "        file_count = len(list(path_obj.glob(\"*\"))) if path_obj.exists() else 0\n",
    "        print(f\"{path_name}: {status} ({file_count} files)\")\n",
    "\n",
    "print(\"\\nPERFORMANCE OPTIMIZATION TIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. For large areas (current: {area:.2f} degree²):\")\n",
    "print(f\"   - Current buffer: {CONFIG['extent']['buffer_degrees']} degrees\")\n",
    "print(f\"   - Parallel processing: {CONFIG['tiling']['parallel']}\")\n",
    "print(\"   - Consider smaller chunks if memory issues occur\")\n",
    "\n",
    "print(\"\\n2. File management:\")\n",
    "print(f\"   - Monitor {CONFIG['paths']['data_dir']} size during processing\")\n",
    "print(\"   - Clean intermediate files between steps if needed\")\n",
    "print(\"   - Use filter patterns to process specific layers only\")\n",
    "\n",
    "print(\"\\n3. Output optimization:\")\n",
    "print(f\"   - PMTiles output: {CONFIG['paths']['tile_dir']}\")\n",
    "# print(f\"   - Public tiles: {CONFIG['paths']['public_tiles_dir']}\")\n",
    "print(\"   - Copy final tiles to public directory for web serving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ee77b",
   "metadata": {},
   "source": [
    "# Modular Processing Summary\n",
    "\n",
    "This notebook provides a complete, step-by-step approach for **large-scale geospatial data processing** optimized for continent and world-scale datasets.\n",
    "\n",
    "## Core Steps\n",
    "1. **Download Overture Maps data** with spatial filtering using DuckDB (outputs GeoParquet)\n",
    "2. **Check and validate** downloaded files\n",
    "3. **Convert to FlatGeobuf** - Optimize GeoParquet for efficient tile generation\n",
    "4. **Convert custom spatial data** to GeoJSON/FlatGeobuf format\n",
    "5. **Generate PMTiles** using optimized tippecanoe settings\n",
    "6. **Create TileJSON metadata** for web mapping integration\n",
    "7. **Validate and test** individual processing steps\n",
    "\n",
    "## Format Workflow (Optimized for Scale)\n",
    "\n",
    "```\n",
    "Download (DuckDB)     Convert           Tile (Tippecanoe)\n",
    "─────────────────     ───────           ─────────────────\n",
    "GeoParquet (.parquet) → FlatGeobuf (.fgb) → PMTiles (.pmtiles)\n",
    "    ↓                     ↓                      ↓\n",
    "  Compact            Streaming Read         Web Optimized\n",
    "  Fast Query         Spatial Index          Vector Tiles\n",
    "  50-80% smaller     Low Memory            HTTP Range Requests\n",
    "```\n",
    "\n",
    "## Why This Workflow?\n",
    "\n",
    "### 1. GeoParquet for Download\n",
    "- **Compact storage**: 50-80% smaller than GeoJSON\n",
    "- **Fast DuckDB queries**: Efficient spatial filtering\n",
    "- **Columnar format**: Excellent compression\n",
    "\n",
    "### 2. FlatGeobuf for Tiling\n",
    "- **Streaming capability**: Process datasets larger than RAM\n",
    "- **Spatial indexing**: R-tree for fast spatial queries\n",
    "- **Native tippecanoe support**: No conversion overhead\n",
    "- **Optimal for large scale**: Tested on continent/world datasets\n",
    "\n",
    "### 3. PMTiles for Serving\n",
    "- **Cloud-native**: Works with any static file host\n",
    "- **Efficient delivery**: HTTP range requests\n",
    "- **No tile server needed**: Direct browser access\n",
    "\n",
    "## Performance Benefits\n",
    "- **Memory efficiency**: Process billions of features without OOM errors\n",
    "- **Disk space**: GeoParquet + FlatGeobuf = 2-3x less than GeoJSON workflow\n",
    "- **Processing speed**: 20-40% faster tile generation vs GeoJSON\n",
    "- **Parallel processing**: Multi-threaded for optimal CPU utilization\n",
    "\n",
    "## Scale Capabilities\n",
    "- ✓ **City-scale**: Brooklyn, Paris, Tokyo\n",
    "- ✓ **Country-scale**: DRC, USA, India  \n",
    "- ✓ **Continent-scale**: Africa, Europe, Americas\n",
    "- ✓ **World-scale**: Global basemaps with billions of features\n",
    "\n",
    "## Key Features\n",
    "- **Modular design** - Each step can be run independently\n",
    "- **Flexible configuration** - Easy to customize for different areas and data types\n",
    "- **Interactive development** - Run steps individually for debugging\n",
    "- **Performance optimized** - Format selection based on dataset size\n",
    "- **Production ready** - Robust error handling and validation\n",
    "- **Memory conscious** - Streaming workflows prevent OOM errors\n",
    "\n",
    "## Output Files\n",
    "Each step generates specific outputs:\n",
    "- **GeoParquet files (.parquet)** - Compact download format\n",
    "- **FlatGeobuf files (.fgb)** - Optimized tiling input (streaming, indexed)\n",
    "- **PMTiles files (.pmtiles)** - Efficient web mapping output\n",
    "- **TileJSON metadata** - MapLibre GL JS integration\n",
    "\n",
    "## Usage Patterns\n",
    "- **Development**: Run steps individually for testing and debugging\n",
    "- **Production**: Execute all steps in sequence for automated processing\n",
    "- **Customization**: Modify CONFIG settings and re-run specific steps\n",
    "- **Integration**: Use generated PMTiles with web mapping applications\n",
    "\n",
    "## Best Practices for Large Datasets\n",
    "1. **Always convert to FlatGeobuf** before tiling (don't tile GeoParquet directly)\n",
    "2. **Use parallel processing** for multi-file datasets\n",
    "3. **Monitor disk space**: Keep both parquet and fgb during processing\n",
    "4. **Clean up intermediate files** after successful tiling (keep parquet as source)\n",
    "5. **Process by region** for extremely large datasets (e.g., split continents into countries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
